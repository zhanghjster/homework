AI需要具有在原始数据中提取模式的能力，这种能力成为机器学习。

简单的机器学习性能很大程度依赖于给定数据的表示(representation)，很多人工智能任务都可以通过先提取特征然后将特征提供给简单的机器学习算法

对于许多任务，很难知道该提取哪些特征，可以通过机器学习来发掘表示本身，称为表示学习（representation learning)。表示学习算法可以在几分钟为简单的任务发现一个很好的特征

设计特征或设计用于学习特征的算法时，目标通常是***分离出能够解释观察数据的变差因素***(facetors of variation)，这些因素通常不能被直接观察到量，但会影响可观测的量。为了对观察数据体统有用的简化解释，还可能以概念形式存在于人类思维，可以被看成数据的概念或抽象。(影响数据的因素，可能以抽象的概念形式)

人工智能困难主要源于多个变差因素影响这我们观察到的数据。比如红色车照片在晚上可能就是黑色了。大多数应用需要理清变差因素，忽略不关心的因素

从原始数据提取高层次的抽象特征非常困难，比如说话口音这样的变差因素，只能通过对数据进行复杂的接近人类水平的理解来辨识，这几乎与获得原问题的标识一样困难，所以乍看标识学习似乎并不能帮助我们

***深度学习***通过其他简单的标识来表达复杂的标识，解决表示学习的核心问题，深度学习让计算机通过简答你的概念构建复杂的概念。

两种度量模型深度的方法，一种是基于评估***架构所需执行顺序指令的数目***，把模式表示为给定输入，计算对应输出的流程图，这张流程图的最长路径。另一种是在深度概率模型中的使用的方法，将***描述概念彼此关联的图的深度***视为模型深度。

深度学习是通向人工智能途径之一，一种能够使计算机从经验和数据中得到提高的技术。深度学习是一种特定类型的机器学习，具有强大的能力和灵活性。将大千世界表示为嵌套的层次概念体系

**深度学习**可以理解为解决能够**自动将复杂特征分解为嵌套的层次概念**的表示学习。

总结：

机器学习首先要提取特征变差因素，在不可能人提取特征的情况下使用表示学习算法提取，但还有一些复杂的抽象无法用表示学习提取时，深度学习使用简单分层嵌套的方法来表示复杂的抽象

#### 历史

控制论(cybernetics) => 连接主义(connectionism) => 深度学习(deap learning)

现代深度学习最早前身是从神经科学角度出发的简单线性模型，这种模型设计为使一组输入，将他们与一个输出关联。但如今神经科学在深度学习研究中作用被削弱，因为我们根本没有足够的关于大脑的信息来作为指导去使用它

从神经科学得到粗略的指南，仅通过计算单元之间的相互作用而变得智能的基本思想是受大脑启发，目前大多数神经网路基于一个称为***整流线性单元***的神经单元模型

联结注意的中心思想是，当网络将大量简单计算单元连接在一起时可以实现智能行为。重要概念包括

* 分布式表示(distributed representation), 系统中的每个输入都应该由多个特征表示，并且每个特征都应该参与到多个可能输入的表示
* 长短期记忆

日益增长的数据量是深度学习发展的重要推动力，一个粗略估计5000个标注样本下，有监督深度学习可以达到一个可以接受的范围，到1000万个标注样本情况下，可以达到人类表现。

深度学习是机器学习的一种方法，在过去几十年的发展中，借鉴了关于人脑、统计学、应用数学的知识。得益于更强大的计算机、更大的数据集能够训练更深的额网络技术



### 线性代数 

* 标量，一个单独的数
* 向量，一列有序排列的数
* 矩阵，二位数组，每个元素由两个索引所确定
* 张量，坐标超过两维的数组

转置(transpose) 将矩阵一对角线为轴的镜像，从左上角到右下角的对角线称为主对角线$(A^T)_{i,j} = A_{j,i}$ ，向量可以看成是只有一列的矩阵，向量的转置可以看成只有一行的矩阵，变量可以看成是只有一个元素的矩阵，标量的转置等于他本身

* 只要矩阵形状一样，可以做矩阵相加，指对应位置的元素相加

* 标量和矩阵相乘，或和矩阵相加，只需要它和矩阵的每个元素相加或相乘

* 向量和矩阵相加为向量和矩阵每一行相加 $C = A + b$ 其中 $C_{i,j} = A_{i,j} + b_j$ 这种写法无需在加法操作前定义一个向量b复制到每一行的而生成的矩阵

* 矩阵相乘, 矩阵A的列数必须和矩阵B的行数相等，如果A为$m \times n$, B为 $n \times p$ 则结果为  $m\times p$ 

  $$\\ C_{i,j} = \sum_{k=1}^{n}A_{i,k}B_{k,j}$$

  A的i行和B的j列每个k相同的元素相乘之和

* 两个向量的点积可以看做矩阵乘机$x^Ty$ 计算矩阵乘机$C_{i,j}$可以看做矩阵A的i行和B的j列的点积

* 矩阵乘积的性质

  * $A(B+C) = AB + AC$
  * $A(BC) = (AB)C$

  连个矩阵的乘积不满足交换律

* 两个向量的点积满足交换律

  $x^Ty = y^Tx$

* 矩阵乘机的转置

  $(AB)^T = B^TA^T$

* 单位矩阵 $I$，维持n维向量不变的矩阵为单位矩阵，主对角线上元素都是1，其他元素都是1的矩阵为单位矩阵

* 矩阵的逆 $A^{-1} ​$ 满足如下条件 $A^{-1}A = I_n​$

  求解 $Ax = b$ 转化为 $x = A^{-1}b$

  $A^{-1}Ax = A^{-1}b$ 

  $I_nx = A{-1}b$

  $x = A^{-1}b$

  是否有解取决于是否存在逆矩阵$A^{-1}$ , 逆矩阵主要作为理论上的工具使用，大多数软件程序中不会使用，因为逆矩阵在计算机上只能表现出有限的精度


* A的列向量看做从原点出发的不同方向，方程有多少个解转化成有多少个方法到达向量b，x向量的每个元素代表沿这些方向走多远

  $$\\ Ax = \sum x_iA:,i$$

  一组向量的线性组合是指每个向量乘以对应标量系数之后的和，（方程组的个数）

  $$\\ \sum c_iv^{(i)}$$

  一组向量的生成子空间是原始向量线性组合后所能抵达的点的集合，（b的集合）

  确定方程组是否有解相当于确定向量***b是否在A的列向量的生成子空间***中。这个特殊的生成子空间被称为A的列空间，或者A的值域

  为了使方程$Ax=b$对于任意 $b \in R^m $  都存在解，我们要求A的列空间构成整个 $R^m$ , 如果有某个点不在A的列空间中，那么该点会让方程没有解。矩阵A的列空间是整个$R^m$的要求意味着A至少有m列，否则A的列空间的维数小于m，假如A是$3\times 2$ ，目标b是3维的，但x只有两维，所以无论x如何修改x，都智能描绘$R^3$的一个平面

  $n \geqslant m$ 是方程有解的必要条件，但不是充分条件，因为如果两个列向量相同则他们的列空间是相同的，这种称为线性相关(linear dependence), 如果一组向量中任意一个向量都不能表示成其他向量的线性组合，则这组向量为线性无关，如果某个向量是一组向量中某些向量的线性组合，那么僵这个向量加入到这组向量后，不会增加这组向量的生成子空间。如果有个矩阵的列空间涵盖整个$R^m$ 那么这个矩阵必须包含至少m个线性无关的向量

  要想方程有切只有一个解，矩阵必须是方阵，并且所有列向量都是线性无关的。一个列向量线性相关的方阵为奇异的，如果矩阵不是方阵或者是奇异的任然可能有解，但不能用逆矩阵求解


  方阵并且列向量彼此线性无关 =》 方程有唯一的解

* 范数(norm), 衡量向量的大小

  二范数：每个元素平方和的开平方

  一范数：绝对值之和

  最大范数：元素绝对值的最大值

  矩阵二范数：每个元素平方和后开平方

* 对角矩阵：

  主对角线上的元素不为0，其他所有位置为0的为**对角矩阵**， $diag(v)​$ 表示由向量v的元素给定的一个对角方阵。计算乘法$diag(v)x​$只需要将x中每个元素 $x_i​$ 放大 $v_i​$ 倍。 $diag(v)^{-1} = diag([1/v_1,1/v_2,…,1/v_n]^T)​$, 通过将一些矩阵限制为对角矩阵可以得到计算代价比较低的算法

  非方阵的对角矩阵没有逆矩阵，乘法Dx会涉及每x中每个元素的缩放

* 对称矩阵转置后和自己相等的矩阵 (symmetric)

  $$\\ A= A^T$$

* 单位向量是具有单位范数的向量，即$\lVert x \rVert_2 = 1   $

* 如果$x^Ty = 0$那么向量x和向量y互相正交，如果两个向量都有非零范数，则两个向量见夹角为90度, 正交并且范数是1的向量为标准正交

* 正交矩阵，行向量和列向量分别标准正交的方阵，即 $A^TA = AA^T = I$ 也就是 $A^{-1} = A^T$ 

* 特征分解，将矩阵分解成一组特征向量和特征值，方阵A的特征向量指与A相乘后相当于对该向量进行缩放的非零向量

  $$\\ Av = \lambda v$$

  其中$$\lambda$$ 是这个特征向量对应的特征值，如果$v$是A的特征向量,那么任何缩放后的向量$sv$也是A的特征向量

  假设A有n个线性无关的特征向量$\{v^{(1)},\cdot \cdot \cdot, v^{(n)} \}$ 对应特征值$\{\lambda_1, \cdot \cdot \cdot , \lambda_n \}$我们将特征向量连接成一个矩阵，每一列是个特征向量，$V = [v^{(1)},\cdot \cdot \cdot,v^{(n)}]$, 特征值连接成一个向量 $ \lambda = [\lambda_1,\cdot\cdot\cdot,\lambda_n]^T$ A的特征分解可以记做

  $$\\ A = Vdiag(\lambda)V^{-1}$$

  构建具有特征值和特征向量的矩阵能够使我们在目标方向上延伸空间，将矩阵分解成特征值和特征向量可以帮助我们分析矩阵特性

  不是每个矩阵都可以分解成特征值和特征向量，有些情况下特征分解不存在。实对称矩阵可以分解成实特征向量和实特征值

  ​	$$\\ A = Q\Lambda Q^T$$

  $Q$是A的特征向量组成的正交矩阵, $\Lambda$ 是对角矩阵，特征值 $\Lambda_{i,i}$对应的特征向量是$Q$的第$i$列，记做 $Q:,i$ ，因为$Q$ 是正交矩阵可以将A看做沿方向 $v^{(i)}$延展$\lambda_i$倍的空间 

  实对称矩阵都有特征分解，但特征分解可能不唯一，所有特征值都是正的矩阵为正定，都是非负则为半正定，都是负数的为负定，非正则为半负定zhi

* ​

### 支持向量机(SupportVector Machine)

目的是学会一个分类函数或者模型将数据库中的数据映射到某一个类。支持向量机是一个二分类分类模型，给定一个样本集合，他用一个超平面把样本进行分割成正反两类。这个超平面要尽最大努力正反之间的间隔最大，分类结果才更可信，对于未知新样本才更好的分类预测




###概率与信息论

###数值计算

### 机器学习基础