AI需要具有在原始数据中提取模式的能力，这种能力成为机器学习。

简单的机器学习性能很大程度依赖于给定数据的表示(representation)，很多人工智能任务都可以通过先提取特征然后将特征提供给简单的机器学习算法

对于许多任务，很难知道该提取哪些特征，可以通过机器学习来发掘表示本身，称为表示学习（representation learning)。表示学习算法可以在几分钟为简单的任务发现一个很好的特征

设计特征或设计用于学习特征的算法时，目标通常是***分离出能够解释观察数据的变差因素***(facetors of variation)，这些因素通常不能被直接观察到量，但会影响可观测的量。为了对观察数据体统有用的简化解释，还可能以概念形式存在于人类思维，可以被看成数据的概念或抽象。(影响数据的因素，可能以抽象的概念形式)

人工智能困难主要源于多个变差因素影响这我们观察到的数据。比如红色车照片在晚上可能就是黑色了。大多数应用需要理清变差因素，忽略不关心的因素

从原始数据提取高层次的抽象特征非常困难，比如说话口音这样的变差因素，只能通过对数据进行复杂的接近人类水平的理解来辨识，这几乎与获得原问题的标识一样困难，所以乍看标识学习似乎并不能帮助我们

***深度学习***通过其他简单的标识来表达复杂的标识，解决表示学习的核心问题，深度学习让计算机通过简答你的概念构建复杂的概念。

两种度量模型深度的方法，一种是基于评估***架构所需执行顺序指令的数目***，把模式表示为给定输入，计算对应输出的流程图，这张流程图的最长路径。另一种是在深度概率模型中的使用的方法，将***描述概念彼此关联的图的深度***视为模型深度。

深度学习是通向人工智能途径之一，一种能够使计算机从经验和数据中得到提高的技术。深度学习是一种特定类型的机器学习，具有强大的能力和灵活性。将大千世界表示为嵌套的层次概念体系

**深度学习**可以理解为解决能够**自动将复杂特征分解为嵌套的层次概念**的表示学习。

总结：

机器学习首先要提取特征变差因素，在不可能人提取特征的情况下使用表示学习算法提取，但还有一些复杂的抽象无法用表示学习提取时，深度学习使用简单分层嵌套的方法来表示复杂的抽象

##### 历史

控制论(cybernetics) => 连接主义(connectionism) => 深度学习(deap learning)

现代深度学习最早前身是从神经科学角度出发的简单线性模型，这种模型设计为使一组输入，将他们与一个输出关联。但如今神经科学在深度学习研究中作用被削弱，因为我们根本没有足够的关于大脑的信息来作为指导去使用它

从神经科学得到粗略的指南，仅通过计算单元之间的相互作用而变得智能的基本思想是受大脑启发，目前大多数神经网路基于一个称为***整流线性单元***的神经单元模型

联结注意的中心思想是，当网络将大量简单计算单元连接在一起时可以实现智能行为。重要概念包括

* 分布式表示(distributed representation), 系统中的每个输入都应该由多个特征表示，并且每个特征都应该参与到多个可能输入的表示
* 长短期记忆

日益增长的数据量是深度学习发展的重要推动力，一个粗略估计5000个标注样本下，有监督深度学习可以达到一个可以接受的范围，到1000万个标注样本情况下，可以达到人类表现。

深度学习是机器学习的一种方法，在过去几十年的发展中，借鉴了关于人脑、统计学、应用数学的知识。得益于更强大的计算机、更大的数据集能够训练更深的额网络技术

##### 线性代数

* 标量，一个单独的数
* 向量，一列有序排列的数
* 矩阵，二位数组，每个元素由两个索引所确定
* 张量，坐标超过两维的数组

转置(transpose) 将矩阵一对角线为轴的镜像，从左上角到右下角的对角线称为主对角线$(A^T)_{i,j} = A_{j,i}$ ，向量可以看成是只有一列的矩阵，向量的转置可以看成只有一行的矩阵，变量可以看成是只有一个元素的矩阵，标量的转置等于他本身

* 只要矩阵形状一样，可以做矩阵相加，指对应位置的元素相加

* 标量和矩阵相乘，或和矩阵相加，只需要它和矩阵的每个元素相加或相乘

* 向量和矩阵相加为向量和矩阵每一行相加 $C = A + b$ 其中 $C_{i,j} = A_{i,j} + b_j$ 这种写法无需在加法操作前定义一个向量b复制到每一行的而生成的矩阵

* 矩阵相乘, 矩阵A的列数必须和矩阵B的行数相等，如果A为$m \times n$, B为 $n \times p$ 则结果为  $m\times p$ 

  $$\\ C_{i,j} = \sum_{k=1}^{n}A_{i,k}B_{k,j}$$

  A的i行和B的j列每个k相同的元素相乘之和

* 两个向量的点积可以看做矩阵乘机$x^Ty$ 计算矩阵乘机$C_{i,j}$可以看做矩阵A的i行和B的j列的点积

* 矩阵乘积的性质

  * $A(B+C) = AB + AC$
  * $A(BC) = (AB)C$

  连个矩阵的乘积不满足交换律

* 两个向量的点积满足交换律

  $x^Ty = y^Tx$

* 矩阵乘机的转置

  $(AB)^T = B^TA^T$

* 单位矩阵 $I$，维持n维向量不变的矩阵为单位矩阵，主对角线上元素都是1，其他元素都是1的矩阵为单位矩阵

* 矩阵的逆 $A^{-1} ​$ 满足如下条件 $A^{-1}A = I_n​$

  求解 $Ax = b$ 转化为 $x = A^{-1}b$

  $A^{-1}Ax = A^{-1}b$ 

  $I_nx = A{-1}b$

  $x = A^{-1}b$

  是否有解取决于是否存在逆矩阵$A^{-1}$ , 逆矩阵主要作为理论上的工具使用，大多数软件程序中不会使用，因为逆矩阵在计算机上只能表现出有限的精度


* A的列向量看做从原点出发的不同方向，方程有多少个解转化成有多少个方法到达向量b，x向量的每个元素代表沿这些方向走多远

  $$\\ Ax = \sum x_iA:,i$$

  一组向量的线性组合是指每个向量乘以对应标量系数之后的和，（方程组的个数）

  $$\\ \sum c_iv^{(i)}$$

  一组向量的生成子空间是原始向量线性组合后所能抵达的点的集合，（b的集合）

  确定方程组是否有解相当于确定向量***b是否在A的列向量的生成子空间***中。这个特殊的生成子空间被称为A的列空间，或者A的值域

  为了使方程$Ax=b$对于任意 $b \in R^m $  都存在解，我们要求A的列空间构成整个 $R^m$ , 如果有某个点不在A的列空间中，那么该点会让方程没有解。矩阵A的列空间是整个$R^m$的要求意味着A至少有m列，否则A的列空间的维数小于m，假如A是$3\times 2$ ，目标b是3维的，但x只有两维，所以无论x如何修改x，都智能描绘$R^3$的一个平面

  $n \geqslant m$ 是方程有解的必要条件，但不是充分条件，因为如果两个列向量相同则他们的列空间是相同的，这种称为线性相关(linear dependence), 如果一组向量中任意一个向量都不能表示成其他向量的线性组合，则这组向量为线性无关，如果某个向量是一组向量中某些向量的线性组合，那么僵这个向量加入到这组向量后，不会增加这组向量的生成子空间。如果有个矩阵的列空间涵盖整个$R^m$ 那么这个矩阵必须包含至少m个线性无关的向量

  要想方程有且只有一个解，矩阵必须是方阵，并且所有列向量都是线性无关的。一个列向量线性相关的方阵为奇异的，如果矩阵不是方阵或者是奇异的任然可能有解，但不能用逆矩阵求解


  方阵并且列向量彼此线性无关 =》 方程有唯一的解

* 范数(norm), 衡量向量的大小

  二范数：每个元素平方和的开平方

  一范数：绝对值之和

  最大范数：元素绝对值的最大值

  矩阵二范数：每个元素平方和后开平方

* 对角矩阵：

  主对角线上的元素不为0，其他所有位置为0的为**对角矩阵**， $diag(v)​$ 表示由向量v的元素给定的一个对角方阵。计算乘法$diag(v)x​$只需要将x中每个元素 $x_i​$ 放大 $v_i​$ 倍。 $diag(v)^{-1} = diag([1/v_1,1/v_2,…,1/v_n]^T)​$, 通过将一些矩阵限制为对角矩阵可以得到计算代价比较低的算法

  非方阵的对角矩阵没有逆矩阵，乘法Dx会涉及每x中每个元素的缩放

* 对称矩阵转置后和自己相等的矩阵 (symmetric)

  $$\\ A= A^T$$

* 单位向量是具有单位范数的向量，即$\lVert x \rVert_2 = 1   $

* 如果$x^Ty = 0$那么向量x和向量y互相正交，如果两个向量都有非零范数，则两个向量见夹角为90度, 正交并且范数是1的向量为标准正交

* 正交矩阵，行向量和列向量分别标准正交的方阵，即 $A^TA = AA^T = I$ 也就是 $A^{-1} = A^T$ 

* 特征分解，将矩阵分解成一组特征向量和特征值，方阵A的特征向量指与A相乘后相当于对该向量进行缩放的非零向量

  $$\\ Av = \lambda v$$

  其中$$\lambda$$ 是这个特征向量对应的特征值，如果$v$是A的特征向量,那么任何缩放后的向量$sv$也是A的特征向量

  假设A有n个线性无关的特征向量$\{v^{(1)},\cdot \cdot \cdot, v^{(n)} \}$ 对应特征值$\{\lambda_1, \cdot \cdot \cdot , \lambda_n \}$我们将特征向量连接成一个矩阵，每一列是个特征向量，$V = [v^{(1)},\cdot \cdot \cdot,v^{(n)}]$, 特征值连接成一个向量 $ \lambda = [\lambda_1,\cdot\cdot\cdot,\lambda_n]^T$ A的特征分解可以记做

  $$\\ A = Vdiag(\lambda)V^{-1}$$

  构建具有特征值和特征向量的矩阵能够使我们在目标方向上延伸空间，将矩阵分解成特征值和特征向量可以帮助我们分析矩阵特性

  不是每个矩阵都可以分解成特征值和特征向量，有些情况下特征分解不存在。实对称矩阵可以分解成实特征向量和实特征值

  ​	$$\\ A = Q\Lambda Q^T$$

  $Q$是A的特征向量组成的正交矩阵, $\Lambda$ 是对角矩阵，特征值 $\Lambda_{i,i}$对应的特征向量是$Q$的第$i$列，记做 $Q:,i$ ，因为$Q$ 是正交矩阵可以将A看做沿方向 $v^{(i)}$延展$\lambda_i$倍的空间 

  实对称矩阵都有特征分解，但特征分解可能不唯一，所有特征值都是正的矩阵为正定，都是非负则为半正定，都是负数的为负定，非正则为半负定zhi

* 行列式对一个矩阵A而言，矩阵A表示一个n维空间的线性变换，也就是压缩或拉伸，假设一个n维的立方体经过矩阵A变换后成一个新立方体，原立方体的体积是$V_1$ 新立方体的体积是$V_2$ 行列式$det(A)$ 就是$V_2 \div  V_1$ 也就是线性变换的放大率

* 空间，容纳运动是空间的本质，不管什么空间，都碧玺容纳支持在其中发生的符合规则的运动（变换），空间是容纳运动的一个对象集合，而变换则规定了对应空间的运动。

* 向量刻画空间的对象，矩阵刻画对象的运动，用矩阵与向量的乘法刻画运动

* 矩阵是线性空间里变换的描述，在线性空间中，只要选定一组基，那么对于任何一个线性变换，都能够用一个确定的矩阵来加以描述，对于一个线性变换，只要选定一组基就可以找到一个矩阵来描述这个线性变换，换一组基就得到不同的矩阵，所有的这些矩阵都是这样同一个线性变换的描述，但不是线性变换本身。

  比如从不同角度录刘翔的一次110米跨栏比赛，角度不同，拍出来的片子不同，但他们反映的都是刘翔的同一次运动。

  那么问题来了，如果知道两断片子描述的是同一次比赛呢？也就是如果知道两个矩阵描述的是同一个线性变换呢？

  若矩阵A与B是同一个线性变换的不同描述，则一定能找到一个非奇异矩阵P使得AB满足

  $$\\ A = P^{-1}BP$$ 

  也就是A B是相似矩阵，即，同一个线性变换的不同描述，上面的P就是A矩阵所用的基与B所用的基之间的变换关系，上面的变换也称相似变换

  ​

* 线性空间的线性变换T在选定一组基后可以表示为一个矩阵

* 奇异值分解，将矩阵分解为奇异向量和奇异值，每个实数矩阵都有奇异值分解

  $$\\ A = UDV^T$$

  假设A为一个$m \times n$ 的矩阵，U是一个$m \times m$ 矩阵，D是$m\times n$, V是 $n \times n$ ，D为对角矩阵，其对角线上的值为A的奇异值，U的列向量为左奇异向量，V的列向量为A的有奇异向量



##### 概率与信息论

概率论使我们能够提出不确定性声明以及不确定性存在的情况下进行推理，信息论使我们能量化概率分布中不确定性的总量

不确定性的三种可能来源：

1. 被建模系统内在随机性
2. 不完全观测
3. 不完全建模

概率与直接事件发生的频率相联系，被称为频率派概率，与确定性相联系称为贝叶斯概率

##### 随机变量（random varivable）

对可能状态的描述，它必须伴随着一个概率分布来指定每个状态的可能性。随机变量可以是离散的也可以是连续的

##### 概率分布 （probability distribution）

用于描述随机变量或一簇随机变量在在每个可能取到的状态的可能性大小。描述概率分布的方式取决于随机变量是离散的还是连续的

离散型随机变量的概率分布可以用概率质量函数(probability mass function, PMF) ，它将随机变量的每个状态映射到该状态的概率，表示方式$P(x)$,多变量的概率分布被称为联合概率分布$P(x,y)$

连续型随机变量我们用概率密度函数(probability density function, PDF),概率密度函数并没有直接对特定状态给出概率，相对的他给出了落在面积为$\delta x$的无限小区域内的概率为$p(x)\delta x$, 用概率函数求积分来获得点击的真是概率质量，比如$x$落在集合$S$中的概率可以通过$p(x)$对这个集合求积分来得到， x落在区间$[a,b]$ 的概率是$\int_{[a,b]}p(x)d(x)$ 

##### 边缘概率

定义在子集上的概率分布成为边缘概率分布(marginal probability distribution)。假设离散型随机变量x，y并且我们知道$P(x,y)$, 求$P(x)$

$$\\ \forall x \in X, P(x) = \sum _yP(x, y)$$

##### 条件概率

某个时间再给今天其他时间发生时出现的概率，这种概率叫条件概率。对于给定X=x时Y=y发生的调价那概率为

$$\\ P(Y=y|X=x) = P(x,y)/P(x)$$

##### 条件概率的链式法则

任何多维随机变量的联合概率分布都可以分解成只有一个变量的条件概率的相乘形式

$$\\ P(x^{(1)},\cdot\cdot\cdot,x^{(n)}) = P(x^{(1)})\Pi_{i=2}^nP(x^{(i)}|x^{(1)},\cdot\cdot\cdot,x^{(i-1)})$$

或者

$$\\ P(x^{(1)},\cdot\cdot\cdot,x^{(n)}) = P(x^n)\Pi_{i=2}^{n-1}P(x^{i}|x_{(i+1)},\cdot\cdot\cdot,x^{(n)})$$

比如

$$\\ P(a,b,c) = P(a|b,c)P(b|c)P(c)$$

##### 独立性和条件独立性

两个随机变量x和y，如果它们的联合概率分布可以表示成两个因子乘机的形式，并且一个因子值包含x另一个因子只包含y，那么这两个变量是相互独立的

$$\\ \forall x \in X, y \in Y, p(x,y) = p(x)p(y) $$

如果关于x和y的条件概率分布对于z的每一个值都可以写成乘机的形式，那么这两个随机变量x和y在给定随机比恩量x时是条件独立的

$$\\ \forall x \in X, y \in Y, z \in Z, p(x,y|z) = p(x|z)p(y|z)$$

简化写法

$$\\x\bot y | z$$

##### 期望、方差、协方差

函数$f(x)$ 关于某分布$P(x)$的期望(expectation)和期望值(expected value)指，当x由P产生，$f$ 作用于x时， $f(x)$的平均值，对于离散型随机变量，可以通过求和得到

​	$$\\ \mathbb{E}_{x\sim P[f(x)]} = \sum_x P(x)f(x)$$

对于连续行变量通过积分得到

$$\\ \mathbb{E}_{x\sim p[f(x)]} = \int p(x)f(x)dx$$

期望是线性的

$$\\ \mathbb{E}_x[\alpha f(x) + \beta g(x) ] = \alpha \mathbb{E}_x[f(x)] + \beta \mathbb{E}_x[g(x)]$$

方差(variance)衡量的是当我们对x依据他的概率分布进行采样时，随机变量x的函数会呈现多大差异

$$\\ Var(f(x)) = \mathbb{E}[(f(x)-  \mathbb{E}[f(x)])^2]$$



可以理解为$f(x)$ 与它期望值距离的平方的均值，当方差很小时$f(x)$的值形成的簇不叫接近他们的期望值，方差的平方根成为标准差

协方差(covariance)在某种意义上给出了两个变量线性相关的强度以及这些变量的尺度

$$\\ Cov(f(x), g(y)) = E[(f(x) - E[f(x)])(g(y) - E[g(y)])])$$

协方差的绝对值越大，意味变量值的变化越大，并且他们同时距离各自的均值很远，如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值，如果是负的，那么一个便利囊倾向于取得较大的值时另一个变量倾向于取的较小的值

如果两个变量相互独立，那么他们的协方差为0，他们之间一定没有线性关系，如果协方差不为零，那么他们的一定是相关的 。但独立性比0协方差要求更强，因为独立性还排除了非线性的关系

##### 常用概率分布

1. Multinoulli分布，也叫范畴分布(categorical distribution), 指具有k个不同状态的单个离散型随机变量上的分布，其中k是个有限值，，有向量 $p \in [0,1]^{k-1}$参数化，其中每个分量 $p_i$表示第i个状态的概率，最后第k个状态的概率可以通过 $1 - 1^Tp$给出，经常用来对所有状态能够进行枚举的离散型随机变量进行建模

2. 高斯分布，也称正太分布(normal distribution)，呈现经典的钟形曲线的形状. 随机变量服从期望值为$\mu$方差为$\sigma ^2$的正太分布，概率密度函数为

   $$\\ f(x) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})$$

   标注正太分布 $\mu = 0, \sigma = 1$

3. 指数分布

4. Dirac分布和经验分布

##### 贝叶斯规则

$$P(x|y)P(y) = P(y|x)P(x)$$

##### 信息论

对一个信号包含信息的多少进行量化，最初发明用来研究在一个含有噪声的信道上用离散的字母来发送消息，比如用无线电来传输通信，在这种情况下，信息论告诉我们如何对消息设计最优编码以及计算消息的期望长度。机器学习中可以把信息论应用于连续性变量，用信息论描述概率分布，或者量化概率之间的相似性

信息论的基本想法是一个不太可能发生的事件发生了，要比一个非常可能的时间发生能够提供更多的信息

一个时间$x$的自信息为

$$\\ I(x) = -log(P(x))$$

底数为$e$ , 单位为奈特(nates), 一奈特是以 $\frac{1}{e}$ 的概率观测到的一个事件获得的信息量，以2位底的对数单位为bit或者香农(shannons), 通过比特度量的信息只是通过奈特度量信息的常数倍

自信息只处理单个的输出，可以用香农熵来对整个概率分布的不确定性总量进行量化， 平均而言发生一个时间我们得到的信息量的大小，数学上，信息熵其实是信息量的期望

$$\\ H(x) = \mathbb{E}_{x\sim P}[I(x)] = - \mathbb {E}_{x\sim P}[logP(x)]$$

一份分布的香农熵是指遵循这个分布的事件所产生的期望信息总量，它给出了一句概率$P$生成的符号进行编码所需的比特数在平均意义上的下界

##### 结构化概率模型

有向和无向

##### 机器学习基础


分类与聚类的区别

1. 分类是已知类别用数据训练学习找到这些类别的不同特征然后对未分类数据进行分类
2. 聚类压根不知道分类，通过聚类分析扎到几个群体，不需要对数据进行训练和学习

##### 线性

1. 线性空间，定义于特定数域上的非空集合，如果上面可以定义两种线性运算man许八条性质，可以成为线性空间
2. 如果一个线性空间的任何一个向量都可以用一组线性无关的向量线性表示，那么这一组向量成为基
3. 线性子空间，
4. 线性映射，当一个映射满足加法和乘法两个基本的线性运算规则，称为线性映射
5. 线性变换，从一个线性空间到他自己本身的一个线性映射


1. 可加性 $f(x+y) = f(x) + f(y)$
2. 齐次性 $f(ax) = af(x)$

线性变换，是用线性函数转换，可以理解为矩阵变换，也就是旋转和拉伸

1. 线性空间

所有的创新性发现都不是靠逻辑推理推出来的，逻辑推理推导的只是已经有的结论

http://blog.csdn.net/yunduanmuxue/article/details/17115205



##### 激活函数

非线性的激活函数将输入转化为非线性的输出，用激活函数实现去线性化



##### 损失函数

定义神经网络模型的效果以及优化目标，分类问题和回归问题是有监督学习的两大种类。分类问题是将不同样本分到事先定义好的类别

神经网络通过交叉熵衡量两个概率分布之间的距离，是分类问题中比较广泛使用的损失函数，验证模型分类的准确程度，·将分类结果用softmax模型转化成概率分布，然后通过交叉熵验证与正确结果之间的距离

回归问题是对具体数值的预测，比如房价，销量等，这些问题预测的不是事先定义好的类别而是一个任意实数，解决回归问题的神经网络一般只有一个输出节点，而这个输出值就是预测的值。常用的损失函数是均方误差MSE(mean squared error)

$$\\ MES(y,y^{'}) = \frac{\sum _{i=1}^n(y_i - y^{'})^2}{n}$$

$y_i$ 为一个batch中第i个数据的正确答案,$y^{'}_i$为预测的值

##### 神经网络优化算法

通过反向传播算法和梯度下降算法对神经网络中的参数取值进行优化，梯度下降算法用于优化当个参数的取值，反向传播算法高效的在所有参数上使用梯度下降算法，从而是神经网络在训练数据上损失函数尽可能的小

学习率控制参数更新的幅度，ts提供了灵活的学习率设置方法，指数衰减法

##### 过拟合问题

当一个模型过为复杂后，他可以很好的记忆每个训练数据中随机噪音而忘记要去歇息训练数据中的通用趋势。使用正则化在损失函数中加入刻画模型复杂度的指标，通过限制权重的大小，是模型不能任意你和随机噪音

##### 上溢与下溢

softmax函数用于预测Multinoulli相关联的概率定义为

$$\\ softmax(x)_i = \frac{exp(x_i)}{\sum _{j=1}^nexp(x_j)}$$

数值计算的方法用于优化参数

##### 机器学习

本质上是应用统计学，更多的关注如何用计算机统计地估计复杂函数，不太关注这些函数提供的置信区间，两种主要统计学方法，概率派估计和贝叶斯推断。大多数深度学习算法都是基于随机梯度下降的算法求解。

对于某类任务$T$和性能度量$P$ ,一个计算机程序可以被认为可以从经验$E$中学习是指，通过经验$E$改进后，它在任务$T$上的性能由度量$P$度量的性能有所提升

##### 任务

通常机器学习的任务定义为机器学习系统应该如何处理样本。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征的集合。经常将样本表示为一个向量，每个元素代表一个特征

* 分类，学习算法返回一个函数 $f: \mathbb{R}^n \to \{1,\cdot\cdot\cdot,k\}$ ,  当$y=f(x)$, 模型将向量$x$做代表的输入分类到$y$所代表的类别。还有一些分类比如$f$输出的是不同类别的概率分布
* 输入缺失分类，输入向量的每个度量不被保证
* 回归，计算机程序需要对给定输入预测数值
* 转录，观测非机构化的数据，转录信息为离散的文本，比如根据文本图片返回文字序列，比如语音识别将一段音频波形转换成文字
* 机器翻译
* 结构化输出
* 异常检测
* 合成采样
* 缺失值填补
* 去噪
* 密度估计或概率质量函数估计
* 其他

##### 性能度量

评估机器学习算法的能力，必须设计其性能的度量，通常性能度量$P$是特定于系统执行的任务

##### 经验

无监督学习，训练含有很多特征的数据集，然后学习出这个数据及上有用的结构性质。通常要学习生成的数据集的整个特概率分布或者改分布的一些有一丝的性质

监督学习，训练含有很多特征的数据集，不过数据集中的样本都有标签。

习惯上人们将回归，分类或者结构化输出的问题成为监督学习，将支持其他任务的密度估计称为无监督学习。

数据集是样本的集合，样本是特征的集合

表示数据集的常用方法是设计矩阵，设计矩阵的每一行包含一个不同的样本，每一列对应不同的特征

##### 线性回归

建立一个系统将向量$x \in \mathbb{R}$ 作为输入，预测标量 $y \in \mathbb{R}$ 作为输出，线性回归的输出是输入的线性函数

$$\\ \hat y = w^Tx$$ 

其中 $w\in \mathbb{R}$ 是参数向量

性能度量

$$\\ MES_{test} = \frac{1}{n}\sum_i(\hat y^{(test)}- y^{(test)})^2$$

通过最小化训练级上的方差来求解$w$ 为一个直观的模型

机器学习的主要挑战是算法必须能够在先前未观测到的新输入上表现良好，这种能力成为泛化(generalization), 通常情况下，训练机器学习模型时，在某个训练集上计算一些被称为训练误差(training error)的度量误差，目的是降低训练误差。新的输入的误差的期望称为泛化误差(generalization error), 通常通过度量在训练集中分离出来的测试集上的性能来评估机器学习模型的泛化误差

神经网络输入的是一个特征向量

决定机器学习算法效果是否好的因素：

* 降低训练误差
* 缩小训练误差和测试误差的差距

欠拟合，指模型不能在训练集上获得足够低的误差。过拟合，训练误差和测试误差之间的距离太大。通常通过控制模型的容量可以控制是否偏向于过拟合或欠拟合。模型的容量通常指其拟合各种函数的能力，容量低的模型可能很难拟合训练集，容量高的模型可能会过拟合。一种控制训练算法容量的方法是假设空间（hypothesis space)， 即学习算法可以选择为解决方案的函数集

没有免费午餐定理表明，在所有可能的数据生成分布上平均后，每个分类算法在未观测点上都有相同的错误率，换言之，没有某种意义上没有一个机器学习算法比其他的要好。幸运的是这种结论在我们考虑所有可能的数据生成分布上才成立，如果我们对遇到的概率分布进行假设，可以设计在这些分布上效果良好的算法

没有免费午餐定理暗示我们必须在特定任务上设计性能良好的算法

算法的效果不仅很大程度上受假设空间的函数数量，也取决于这些函数的形式，比如用线性函数拟合$\sin(x)$ 效果就不好。

正则化(regularization)，修改学习算法，使其降低泛化误差而非训练误差，正则化是机器学习领域的中心问题之一

没有免费午餐定理说明没有最优的学习算法，特别是没有最优的正则化形式，反之，我们必须挑选一个非常适合我们所要解决的任务的正则形式。深度学习中的普遍理念是大量任务也许都可以用非常通用的正则化形式来有效解决

##### 超参数和验证集

大多数机器学习算法都有超参数，用来控制算法行为，他的值不是通过学习算法本身学习出来的。用于挑选超参数的数据自己被称为验证集。通常$80\%$的训练数据用于训练, $20\%$用于验证

##### 交叉验证

将数据集分成固定的训练集和固定的测试集后，若测集误差很小，这将是有问题的。一个小规模的测试集意味着平均测试误差估计的统计不确定性，很难判断算法A是否比算法B好

当数据集太小时，替代方法是将集合非诚k个不重合的自己，测试误差可以估计为k次计算后的平均测试误差，在第i次测试时，数据的第i个自己用于测试，其他用于训练

##### 点估计

试图为感兴趣的量提供单个的最优预测，一般的是单个参数，或者是某个参数模型中的一个向量参数，也有可能是一个函数

习惯将参数 $\theta$ 的点估计表示为 $\hat{\theta}$ , 另$\{x^{(1)}, \cdot,\cdot\cdot,x^{(m)}\}$  是m个独立分布的数据点。点估计或统计量是这些数据的任意函数

$$\\ \hat{\theta} = g(x^{(1)},\cdot\cdot\cdot,x^{(m)})$$ 

这里不要求$g$返回一个接近真实的$\theta$的值，但良好的估计量的输出会接近生成训练数据的真实参数$\theta$

##### 函数估计

试图从输入向量$x​$预测变量$y​$ ,假设有个函数$f(x)​$表示$y​$和$x​$之间的近似关系，比如假设 $y = f(x) + \epsilon​$  其中 $\epsilon​$ 是y中为能从x预测的一部分，用一个模型不急去近似$f​$ 

##### 偏差

估计的偏差

$$\\ bias(\hat{\theta_m}) = \mathbb{E}(\hat{\theta_m}) - \theta$$ 

其中期望组用于所有数据上，$\theta$ 是用于定义数据生成分布的真实值

 ##### 方差和标准差

衡量估计值作为样本的函数，期望的变化程度

偏差度两者偏离真实函数或参数的误差，方差度量着数据上任意特定采样可能导致的估计期望的偏差

##### 最大似然估计

做点估计或函数估计的准则

估计的原则，最大似然法，衡量手段，偏差、方差、标准差

##### 贝叶斯统计

概率派认为真实参数$\theta$是未知的定值，而点估计 $\hat{\theta}$ 是考虑数据及上的函数的随机变量。贝叶斯派认为概率反应只是状态的确定性程度，数据集能被直接观测到，因此不是随机的，而真实参数$\theta$是未知或不确定的，因此可以表示成随机变量

##### 深度前馈网络

Deep feedforward network，也叫前馈神经网络(feedforward neural network)或多层感知机 （multplayer preceptron, MLP)。 前馈网络定义了一个映射函数 $y = f(x;\theta)$ ，并且学习参数$\theta$的值，使它能够得到最佳函数近似

前馈神经网络由血多不同的函数符合而成，它与一个有向无环图像关联，而图描述了函数是如何复合在一起的。网络中的每个影藏曾通常都是向量，它的位数决定的模型的宽度，向量的每个元素都被视为类似一个神经元，它接收来自其它层的多个输入，产生一个输出



参考:

1. 支持向量机、逻辑回归、决策树、朴素贝叶斯分类器、随机森林、聚类算法、协同过滤、关联性分析、人工神经网络、BP算法、PCA、过拟合、正则化





